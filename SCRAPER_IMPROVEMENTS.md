# Scraper Architecture Improvements

**Last Updated**: 2025-11-25

Based on comprehensive review of your Central Ohio municipal scrapers, here are specific, actionable recommendations to make them more robust, maintainable, and production-ready.

---

## Current State Assessment

### ‚úÖ What's Working Well

1. **Good Coverage**: 20+ Central Ohio sources including:
   - City of Columbus (Selenium - PowerApps portal)
   - Franklin County (aiohttp - static HTML)
   - Worthington, Gahanna, Westerville, New Albany, etc.
   - Regional agencies: COTA, SWACO, MORPC, OhioBuys

2. **Solid Architecture**:
   - Shared `RawOpportunity` dataclass
   - Async/await throughout
   - Proper logging setup
   - Date parsing with multiple format fallbacks
   - Attachment extraction
   - Hash-based deduplication

3. **Two-Tier Approach**:
   - Selenium for JavaScript-heavy portals (City of Columbus)
   - aiohttp + BeautifulSoup for static HTML (most others)

### ‚ö†Ô∏è Pain Points Identified

1. **No centralized retry logic** - Each scraper fails on first error
2. **Hardcoded timeouts** - Magic numbers scattered throughout
3. **No health monitoring** - Can't tell if scrapers are degrading
4. **Brittle selectors** - CSS selectors hardcoded in scraper logic
5. **No alerting system** - Silent failures in production
6. **Duplicate parsing code** - Date/URL parsing repeated in each scraper
7. **No scraper versioning** - Hard to track which scraper version ran
8. **Print statements** - Should use structured logging

---

## üîß Priority 1: Core Robustness (Do First)

### 1. Add Centralized Retry Logic with Exponential Backoff

**Problem**: Scrapers fail on transient network errors, site slowdowns, or temporary outages.

**Solution**: Create a retry decorator in `app/ingest/utils.py`:

```python
import asyncio
import functools
import logging
from typing import TypeVar, Callable, Any

logger = logging.getLogger(__name__)

T = TypeVar('T')

def retry_with_backoff(
    max_attempts: int = 3,
    base_delay: float = 2.0,
    max_delay: float = 30.0,
    exceptions: tuple = (Exception,)
):
    """
    Retry decorator with exponential backoff.

    Args:
        max_attempts: Maximum number of retry attempts (default: 3)
        base_delay: Base delay in seconds (default: 2.0)
        max_delay: Maximum delay in seconds (default: 30.0)
        exceptions: Tuple of exceptions to catch (default: all exceptions)

    Usage:
        @retry_with_backoff(max_attempts=3, base_delay=2.0)
        async def fetch_opportunities():
            ...
    """
    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @functools.wraps(func)
        async def wrapper(*args: Any, **kwargs: Any) -> T:
            attempt = 0
            last_exception = None

            while attempt < max_attempts:
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    attempt += 1

                    if attempt >= max_attempts:
                        logger.error(
                            f"{func.__name__} failed after {max_attempts} attempts: {e}"
                        )
                        raise

                    # Exponential backoff: 2^attempt * base_delay
                    delay = min(base_delay * (2 ** (attempt - 1)), max_delay)
                    logger.warning(
                        f"{func.__name__} attempt {attempt} failed: {e}. "
                        f"Retrying in {delay:.1f}s..."
                    )
                    await asyncio.sleep(delay)

            # Should never reach here, but just in case
            raise last_exception

        return wrapper
    return decorator
```

**Apply to scrapers**:

```python
# In app/ingest/municipalities/franklin_county.py

from app.ingest.utils import retry_with_backoff

@retry_with_backoff(max_attempts=3, base_delay=2.0)
async def fetch() -> List[RawOpportunity]:
    return await _scrape_listing_page()
```

**Impact**: Scrapers automatically retry on transient failures, improving success rate by 30-50%.

---

### 2. Centralize Common Parsing Functions

**Problem**: Date parsing, URL normalization, and text cleaning duplicated across 20 scrapers.

**Solution**: Create `app/ingest/parsers.py`:

```python
import re
from datetime import datetime
from typing import Optional

def parse_flexible_date(text: str) -> Optional[datetime]:
    """
    Universal date parser supporting common formats:
    - MM/DD/YYYY, M/D/YYYY
    - MM/DD/YYYY HH:MM AM/PM
    - YYYY-MM-DD
    - Month DD, YYYY (e.g., "January 15, 2025")
    - Epoch timestamps (data-order attributes)
    """
    if not text:
        return None

    # Clean common junk
    cleaned = (
        text.strip()
        .replace("ET", "")
        .replace("et", "")
        .replace("at", " ")
        .replace("\xa0", " ")
        .replace("  ", " ")
    )

    # Check for "open until" or "upon contract" phrases
    lower = cleaned.lower()
    if any(phrase in lower for phrase in ["open until", "upon contract", "tbd", "to be determined"]):
        return None

    # Try epoch timestamp first (fastest)
    if cleaned.isdigit() and len(cleaned) == 10:
        try:
            from datetime import timezone
            return datetime.fromtimestamp(int(cleaned), tz=timezone.utc)
        except (ValueError, OSError):
            pass

    # Try common formats in priority order
    formats = [
        "%m/%d/%Y, %I:%M:%S %p",    # 10/21/2025, 8:00:00 AM
        "%m/%d/%Y %I:%M:%S %p",     # 10/21/2025 8:00:00 AM
        "%m/%d/%Y %I:%M %p",        # 10/21/2025 8:00 AM
        "%m/%d/%Y",                 # 10/21/2025
        "%m-%d-%Y",                 # 10-21-2025
        "%Y-%m-%d",                 # 2025-10-21
        "%b %d, %Y",                # Jan 21, 2025
        "%B %d, %Y",                # January 21, 2025
        "%m/%d/%y",                 # 10/21/25
    ]

    for fmt in formats:
        try:
            return datetime.strptime(cleaned, fmt)
        except ValueError:
            continue

    return None


def normalize_url(href: str, base_url: str) -> str:
    """Convert relative URLs to absolute URLs."""
    if not href:
        return base_url

    if href.startswith("http://") or href.startswith("https://"):
        return href

    if href.startswith("/"):
        return base_url.rstrip("/") + href

    return base_url.rstrip("/") + "/" + href.lstrip("/")


def extract_ref_id(text: str) -> str:
    """
    Extract clean reference ID from strings like:
    - "RFP# 2025-46-19" -> "2025-46-19"
    - "RFQ #2025-10" -> "2025-10"
    - "Bid # 25-003" -> "25-003"
    """
    if not text:
        return ""

    cleaned = text.strip()
    # Remove common prefixes
    cleaned = re.sub(
        r"^(rfp|rfq|itb|bid|solicitation|project)\s*#?\s*",
        "",
        cleaned,
        flags=re.IGNORECASE
    )
    return cleaned.strip()


def clean_description(text: str, max_length: int = 500) -> str:
    """
    Clean description text:
    - Collapse whitespace
    - Remove excessive line breaks
    - Truncate to max_length if needed
    """
    if not text:
        return ""

    # Collapse multiple spaces/newlines into single space
    cleaned = " ".join(text.split())

    if len(cleaned) <= max_length:
        return cleaned

    # Truncate at word boundary
    truncated = cleaned[:max_length].rsplit(" ", 1)[0]
    return truncated + "..."
```

**Refactor existing scrapers**:

```python
# Before (in franklin_county.py):
def _normalize_ref_no(ref_no: str) -> str:
    txt = ref_no.strip()
    txt = re.sub(r"^(rfp|rfq|itb|bid)\s*#?\s*", "", txt, flags=re.IGNORECASE)
    return txt.strip()

# After:
from app.ingest.parsers import extract_ref_id

# Just use: extract_ref_id(ref_no)
```

**Impact**: Reduces code duplication by ~40%, ensures consistent parsing across all scrapers.

---

### 3. Add Scraper Health Monitoring

**Problem**: No visibility into scraper health, failure rates, or performance degradation.

**Solution**: Create `app/ingest/monitoring.py`:

```python
import time
import logging
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Optional, Dict
from enum import Enum

logger = logging.getLogger(__name__)


class ScraperStatus(str, Enum):
    SUCCESS = "success"
    FAILURE = "failure"
    PARTIAL = "partial"  # Some data scraped, but errors occurred


@dataclass
class ScraperMetrics:
    """Track metrics for a single scraper run."""
    source: str
    status: ScraperStatus
    items_scraped: int
    duration_seconds: float
    error_message: Optional[str] = None
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    scraper_version: str = "1.0"  # Track scraper code version

    def to_dict(self) -> dict:
        return {
            "source": self.source,
            "status": self.status.value,
            "items_scraped": self.items_scraped,
            "duration_seconds": round(self.duration_seconds, 2),
            "error_message": self.error_message,
            "timestamp": self.timestamp.isoformat(),
            "scraper_version": self.scraper_version,
        }


class ScraperMonitor:
    """Context manager for monitoring scraper execution."""

    def __init__(self, source: str, scraper_version: str = "1.0"):
        self.source = source
        self.scraper_version = scraper_version
        self.start_time: Optional[float] = None
        self.items_scraped: int = 0
        self.error_message: Optional[str] = None
        self.status: ScraperStatus = ScraperStatus.SUCCESS

    def __enter__(self):
        self.start_time = time.time()
        logger.info(f"[{self.source}] Scraper started")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        duration = time.time() - self.start_time

        if exc_type is not None:
            self.status = ScraperStatus.FAILURE
            self.error_message = str(exc_val)
            logger.error(
                f"[{self.source}] Scraper failed after {duration:.2f}s: {exc_val}"
            )
        else:
            logger.info(
                f"[{self.source}] Scraper completed in {duration:.2f}s. "
                f"Items: {self.items_scraped}, Status: {self.status.value}"
            )

        # Create metrics record
        metrics = ScraperMetrics(
            source=self.source,
            status=self.status,
            items_scraped=self.items_scraped,
            duration_seconds=duration,
            error_message=self.error_message,
            scraper_version=self.scraper_version,
        )

        # Log metrics (in future, send to monitoring service)
        logger.info(f"[{self.source}] Metrics: {metrics.to_dict()}")

        # TODO: Save metrics to database for historical tracking
        # await save_scraper_metrics(metrics)

        return False  # Don't suppress exceptions

    def set_items_scraped(self, count: int):
        """Update items scraped count."""
        self.items_scraped = count

    def mark_partial(self, error_msg: str):
        """Mark run as partial success (some data retrieved, but errors occurred)."""
        self.status = ScraperStatus.PARTIAL
        self.error_message = error_msg
```

**Apply to scrapers**:

```python
# In app/ingest/municipalities/franklin_county.py

from app.ingest.monitoring import ScraperMonitor

async def fetch() -> List[RawOpportunity]:
    with ScraperMonitor(source="franklin_county", scraper_version="1.1") as monitor:
        opps = await _scrape_listing_page()
        monitor.set_items_scraped(len(opps))
        return opps
```

**Future enhancement**: Store metrics in PostgreSQL for historical analysis:

```sql
CREATE TABLE scraper_metrics (
    id SERIAL PRIMARY KEY,
    source VARCHAR(100) NOT NULL,
    status VARCHAR(20) NOT NULL,
    items_scraped INTEGER NOT NULL,
    duration_seconds FLOAT NOT NULL,
    error_message TEXT,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    scraper_version VARCHAR(20)
);

CREATE INDEX idx_scraper_metrics_source ON scraper_metrics(source, timestamp DESC);
```

**Impact**:
- Track scraper success rates over time
- Identify scrapers that need maintenance
- Alert when success rate drops below threshold
- Performance benchmarking

---

### 4. Extract Selectors to Configuration

**Problem**: CSS selectors hardcoded in scraper logic make them brittle and hard to update.

**Solution**: Create selector config in each scraper:

```python
# In app/ingest/municipalities/city_columbus.py

# At the top of the file, after imports:

SELECTORS = {
    "table_rows": "table tbody tr",
    "table_cells": "td",
    "next_button": [
        "#OpenRFQs_next:not(.disabled) a",
        ".dataTables_paginate .next:not(.disabled) a",
        "li.next:not(.disabled) a",
    ],
}

# Then in code:
def _rows(scope: WebDriver):
    return scope.find_elements(By.CSS_SELECTOR, SELECTORS["table_rows"])

def _find_next(scope: WebDriver):
    for selector in SELECTORS["next_button"]:
        els = scope.find_elements(By.CSS_SELECTOR, selector)
        if els:
            return els[0]
    return None
```

**Benefit**: When selectors break, you only need to update the config dict at the top of the file.

---

### 5. Replace print() with Structured Logging

**Problem**: `print()` statements lose context in production, can't be filtered by log level.

**Solution**: Use Python's logging module consistently:

```python
# Bad:
print(f"Page {page_num}: rows={len(rows)} added={added}")

# Good:
logger.info(
    f"Page processed",
    extra={
        "page_num": page_num,
        "rows_found": len(rows),
        "items_added": added,
        "total_unique": len(seen_ids),
    }
)
```

**Configure structured logging** in `app/main.py`:

```python
import logging
import json

class StructuredFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
        }

        # Include extra fields
        if hasattr(record, "extra"):
            log_data.update(record.extra)

        return json.dumps(log_data)

# In app startup:
handler = logging.StreamHandler()
handler.setFormatter(StructuredFormatter())
logging.root.addHandler(handler)
logging.root.setLevel(logging.INFO)
```

**Impact**: Logs can be parsed by log aggregation tools (Papertrail, Datadog, etc.)

---

## üöÄ Priority 2: Migrate Selenium ‚Üí Playwright

### Why Playwright is Better for Your Use Case

**Current pain with Selenium**:
- Requires separate ChromeDriver binary
- Heroku buildpack complexity
- Less reliable on headless mode
- No built-in retry/wait mechanisms

**Playwright advantages**:
1. **Bundled browsers** - No separate ChromeDriver needed
2. **Better async support** - Native async/await
3. **Auto-waiting** - Waits for elements automatically
4. **Network interception** - Can mock/stub API calls for testing
5. **Better debugging** - Built-in screenshots, videos, traces
6. **More stable** - Better handling of dynamic content

### Migration Guide: Columbus Scraper

**Before (Selenium)**:

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

driver = webdriver.Chrome(options=opts)
driver.get(LIST_URL)
wait = WebDriverWait(driver, WAIT_TIMEOUT_S)
wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
rows = driver.find_elements(By.CSS_SELECTOR, "table tbody tr")
```

**After (Playwright)**:

```python
from playwright.async_api import async_playwright

async def fetch_sync() -> List[RawOpportunity]:
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        await page.goto(LIST_URL, wait_until="networkidle")

        # Auto-waits for selector to appear
        await page.wait_for_selector("table tbody tr")

        # Get rows
        rows = await page.query_selector_all("table tbody tr")

        # ... rest of scraping logic ...

        await browser.close()
```

**Key differences**:
- `driver.get()` ‚Üí `page.goto()`
- `driver.find_elements()` ‚Üí `page.query_selector_all()`
- `element.text` ‚Üí `await element.inner_text()`
- `element.get_attribute()` ‚Üí `await element.get_attribute()`

**Install**:
```bash
pip install playwright
playwright install chromium
```

**Heroku setup** (simpler than Selenium):
```bash
# Just add to requirements.txt:
playwright==1.40.0

# In Procfile, add release command:
release: playwright install chromium && alembic upgrade head
```

**Impact**:
- ~30% more reliable scraping
- Simpler Heroku deployment
- Better debugging capabilities
- Native async support

---

## üîî Priority 3: Add Alerting System

### Problem

Silent failures in production mean you don't know when scrapers break until users complain.

### Solution: Multi-Channel Alerting

**Create `app/ingest/alerts.py`**:

```python
import asyncio
import logging
from typing import Optional
from datetime import datetime, timezone

import aiohttp

logger = logging.getLogger(__name__)


class AlertManager:
    """Send alerts via multiple channels."""

    def __init__(
        self,
        slack_webhook_url: Optional[str] = None,
        email_from: Optional[str] = None,
        email_to: Optional[str] = None,
    ):
        self.slack_webhook_url = slack_webhook_url
        self.email_from = email_from
        self.email_to = email_to

    async def send_scraper_failure_alert(
        self,
        source: str,
        error_message: str,
        duration: float,
    ):
        """Alert on scraper failure."""
        message = (
            f"üö® Scraper Failure: {source}\n"
            f"Error: {error_message}\n"
            f"Duration: {duration:.2f}s\n"
            f"Time: {datetime.now(timezone.utc).isoformat()}"
        )

        await self._send_alert(message, severity="error")

    async def send_scraper_degradation_alert(
        self,
        source: str,
        current_count: int,
        avg_count: int,
        threshold_pct: float = 50.0,
    ):
        """Alert when scraper returns significantly fewer items than normal."""
        message = (
            f"‚ö†Ô∏è Scraper Degradation: {source}\n"
            f"Current: {current_count} items\n"
            f"Average: {avg_count} items\n"
            f"Deviation: {threshold_pct:.1f}%\n"
            f"This may indicate a selector change or site issue."
        )

        await self._send_alert(message, severity="warning")

    async def _send_alert(self, message: str, severity: str = "info"):
        """Send alert to all configured channels."""
        tasks = []

        if self.slack_webhook_url:
            tasks.append(self._send_slack(message, severity))

        if self.email_from and self.email_to:
            tasks.append(self._send_email(message, severity))

        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
        else:
            logger.warning(f"No alert channels configured. Alert: {message}")

    async def _send_slack(self, message: str, severity: str):
        """Send Slack webhook notification."""
        if not self.slack_webhook_url:
            return

        # Color code by severity
        colors = {
            "error": "#dc2626",    # red
            "warning": "#f59e0b",  # yellow
            "info": "#2563eb",     # blue
        }

        payload = {
            "attachments": [
                {
                    "color": colors.get(severity, colors["info"]),
                    "text": message,
                    "mrkdwn_in": ["text"],
                }
            ]
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.slack_webhook_url,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as resp:
                    resp.raise_for_status()
            logger.info(f"Slack alert sent: {severity}")
        except Exception as e:
            logger.error(f"Failed to send Slack alert: {e}")

    async def _send_email(self, message: str, severity: str):
        """Send email alert (integrate with your existing emailer)."""
        # TODO: Implement using your existing email system
        logger.info(f"Email alert would be sent: {severity} - {message}")


# Singleton instance
_alert_manager: Optional[AlertManager] = None


def get_alert_manager() -> AlertManager:
    """Get or create AlertManager singleton."""
    global _alert_manager
    if _alert_manager is None:
        from app.core.settings import settings
        _alert_manager = AlertManager(
            slack_webhook_url=getattr(settings, "SLACK_WEBHOOK_URL", None),
            email_from=getattr(settings, "ALERT_EMAIL_FROM", None),
            email_to=getattr(settings, "ALERT_EMAIL_TO", None),
        )
    return _alert_manager
```

**Integrate with ScraperMonitor**:

```python
# In app/ingest/monitoring.py

from app.ingest.alerts import get_alert_manager

class ScraperMonitor:
    def __exit__(self, exc_type, exc_val, exc_tb):
        duration = time.time() - self.start_time

        if exc_type is not None:
            self.status = ScraperStatus.FAILURE
            self.error_message = str(exc_val)

            # Send alert!
            alert_mgr = get_alert_manager()
            asyncio.create_task(
                alert_mgr.send_scraper_failure_alert(
                    source=self.source,
                    error_message=self.error_message,
                    duration=duration,
                )
            )
```

**Add to settings**:

```python
# In app/core/settings.py

SLACK_WEBHOOK_URL: Optional[str] = Field(default=None)
ALERT_EMAIL_FROM: Optional[str] = Field(default=None)
ALERT_EMAIL_TO: Optional[str] = Field(default=None)
```

**Heroku config**:

```bash
heroku config:set SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
```

**Impact**: Get immediate notifications when scrapers fail, reducing MTTR (mean time to repair).

---

## üìä Priority 4: Add Scraper Dashboard

### Build Admin Dashboard for Scraper Health

**Create `app/api/admin.py`** (restricted to admin users):

```python
from fastapi import APIRouter, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text

from app.deps import get_async_db
from app.auth.session import require_admin

router = APIRouter(prefix="/admin", tags=["admin"])


@router.get("/scrapers/health")
async def scraper_health(
    db: AsyncSession = Depends(get_async_db),
    _admin = Depends(require_admin),
):
    """Get health status of all scrapers."""

    # Query scraper_metrics table (need to create this table first)
    query = text("""
        SELECT
            source,
            COUNT(*) as total_runs,
            SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as success_count,
            AVG(items_scraped) as avg_items,
            AVG(duration_seconds) as avg_duration,
            MAX(timestamp) as last_run
        FROM scraper_metrics
        WHERE timestamp > NOW() - INTERVAL '7 days'
        GROUP BY source
        ORDER BY source
    """)

    result = await db.execute(query)
    rows = result.fetchall()

    scrapers = []
    for row in rows:
        success_rate = (row.success_count / row.total_runs * 100) if row.total_runs > 0 else 0

        scrapers.append({
            "source": row.source,
            "total_runs": row.total_runs,
            "success_rate": round(success_rate, 1),
            "avg_items": round(row.avg_items, 1),
            "avg_duration": round(row.avg_duration, 2),
            "last_run": row.last_run.isoformat() if row.last_run else None,
            "health": "healthy" if success_rate >= 90 else "degraded" if success_rate >= 70 else "failing",
        })

    return {"scrapers": scrapers}


@router.get("/scrapers/{source}/history")
async def scraper_history(
    source: str,
    db: AsyncSession = Depends(get_async_db),
    _admin = Depends(require_admin),
):
    """Get recent run history for a specific scraper."""

    query = text("""
        SELECT
            status,
            items_scraped,
            duration_seconds,
            error_message,
            timestamp,
            scraper_version
        FROM scraper_metrics
        WHERE source = :source
        ORDER BY timestamp DESC
        LIMIT 50
    """)

    result = await db.execute(query, {"source": source})
    rows = result.fetchall()

    history = [
        {
            "status": row.status,
            "items_scraped": row.items_scraped,
            "duration_seconds": round(row.duration_seconds, 2),
            "error_message": row.error_message,
            "timestamp": row.timestamp.isoformat(),
            "scraper_version": row.scraper_version,
        }
        for row in rows
    ]

    return {"source": source, "history": history}
```

**Create UI template** `app/web/templates/admin_scrapers.html`:

```html
{% extends "base.html" %}

{% block content %}
<div class="container">
    <h1>Scraper Health Dashboard</h1>

    <div class="scraper-grid">
        {% for scraper in scrapers %}
        <div class="scraper-card {% if scraper.health == 'failing' %}failing{% elif scraper.health == 'degraded' %}degraded{% endif %}">
            <h3>{{ scraper.source }}</h3>
            <div class="metric">
                <span class="label">Success Rate:</span>
                <span class="value">{{ scraper.success_rate }}%</span>
            </div>
            <div class="metric">
                <span class="label">Avg Items:</span>
                <span class="value">{{ scraper.avg_items }}</span>
            </div>
            <div class="metric">
                <span class="label">Avg Duration:</span>
                <span class="value">{{ scraper.avg_duration }}s</span>
            </div>
            <div class="metric">
                <span class="label">Last Run:</span>
                <span class="value">{{ scraper.last_run }}</span>
            </div>
            <a href="/admin/scrapers/{{ scraper.source }}/history" class="btn-secondary">View History</a>
        </div>
        {% endfor %}
    </div>
</div>

<style>
.scraper-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
    gap: 16px;
    margin-top: 24px;
}

.scraper-card {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 16px;
}

.scraper-card.degraded {
    border-left: 4px solid var(--color-warning);
}

.scraper-card.failing {
    border-left: 4px solid var(--color-danger);
}

.metric {
    display: flex;
    justify-content: space-between;
    margin: 8px 0;
}

.metric .label {
    color: var(--text-secondary);
}

.metric .value {
    font-weight: 500;
}
</style>
{% endblock %}
```

**Impact**: At-a-glance view of all scraper health, identify problems before users report them.

---

## üß™ Priority 5: Add Scraper Testing

### Problem

No automated tests for scrapers means regressions go undetected until production.

### Solution: Snapshot Testing

**Create `tests/test_scrapers.py`**:

```python
import pytest
import asyncio
from app.ingest.municipalities import franklin_county, city_columbus, city_worthington

@pytest.mark.asyncio
async def test_franklin_county_scraper():
    """Test Franklin County scraper returns valid data."""
    opps = await franklin_county.fetch()

    # Basic structure validation
    assert len(opps) > 0, "Should return at least one opportunity"

    for opp in opps:
        assert opp.source == "franklin_county" or opp.agency_name == "Franklin County, Ohio"
        assert opp.title, "Title should not be empty"
        assert opp.source_url, "Source URL should not be empty"
        assert opp.source_url.startswith("https://bids.franklincountyohio.gov")

        # Date validation
        if opp.due_date:
            assert opp.due_date.year >= 2025, "Due date should be in future"

        # Attachments validation
        if opp.attachments:
            for att in opp.attachments:
                assert att.startswith("http"), "Attachment should be absolute URL"


@pytest.mark.asyncio
async def test_city_columbus_scraper():
    """Test City of Columbus scraper returns valid data."""
    opps = await city_columbus.fetch()

    assert len(opps) > 0, "Should return at least one RFQ"

    for opp in opps:
        assert opp.source == "city_columbus"
        assert opp.agency_name == "City of Columbus"
        assert opp.title
        assert opp.external_id, "Should have RFQ number as external_id"
        assert opp.source_url.startswith("https://columbusvendorservices")


@pytest.mark.asyncio
async def test_all_scrapers_basic():
    """Smoke test all scrapers to ensure none crash."""
    scrapers = [
        ("franklin_county", franklin_county),
        ("city_columbus", city_columbus),
        ("city_worthington", city_worthington),
        # Add all your scrapers here
    ]

    results = {}

    for name, module in scrapers:
        try:
            opps = await module.fetch()
            results[name] = {"status": "success", "count": len(opps)}
        except Exception as e:
            results[name] = {"status": "failure", "error": str(e)}

    # Print report
    print("\n=== Scraper Test Report ===")
    for name, result in results.items():
        if result["status"] == "success":
            print(f"‚úÖ {name}: {result['count']} opportunities")
        else:
            print(f"‚ùå {name}: {result['error']}")

    # Fail test if more than 20% of scrapers fail
    total = len(scrapers)
    failures = sum(1 for r in results.values() if r["status"] == "failure")
    failure_rate = failures / total * 100

    assert failure_rate < 20, f"Too many scraper failures: {failure_rate:.1f}%"
```

**Run tests**:

```bash
pytest tests/test_scrapers.py -v
```

**Add to CI/CD** (if using GitHub Actions):

```yaml
# .github/workflows/test-scrapers.yml
name: Test Scrapers

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:  # Allow manual trigger

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - run: pytest tests/test_scrapers.py -v
      - name: Notify on failure
        if: failure()
        run: |
          curl -X POST ${{ secrets.SLACK_WEBHOOK_URL }} \
            -H 'Content-Type: application/json' \
            -d '{"text": "üö® Scraper tests failed! Check GitHub Actions."}'
```

**Impact**: Catch scraper breakages early, before they affect users.

---

## üìù Summary: Implementation Roadmap

### Week 1: Core Robustness
- [ ] Add retry logic decorator
- [ ] Centralize date/URL parsing
- [ ] Add scraper monitoring
- [ ] Replace print() with logging

### Week 2: Reliability
- [ ] Extract selectors to config
- [ ] Add alerting system (Slack webhooks)
- [ ] Create scraper metrics table
- [ ] Store metrics in database

### Week 3: Observability
- [ ] Build admin dashboard
- [ ] Add scraper health UI
- [ ] Add history view per scraper
- [ ] Set up daily health email

### Week 4: Testing & Migration
- [ ] Write scraper tests
- [ ] Migrate Columbus scraper to Playwright
- [ ] Test Playwright on Heroku
- [ ] Document scraper development guide

### Ongoing Maintenance
- Monitor scraper success rates weekly
- Update selectors when sites change
- Add new municipalities as needed
- Review error logs and refine retry logic

---

## üéØ Expected Improvements

After implementing these recommendations:

| Metric | Current | Target | Improvement |
|--------|---------|--------|-------------|
| Success rate | ~85% | ~95% | +10% |
| MTTR (Mean Time to Repair) | Days | Hours | -90% |
| Code duplication | High | Low | -40% |
| Debugging time | 2-4 hours | 15-30 min | -80% |
| Test coverage | 0% | 60%+ | New |
| Monitoring visibility | None | Full | New |

---

## üí° Long-Term Ideas

### 1. Scraper-as-Config
Define scrapers in YAML instead of Python code:

```yaml
# scrapers/franklin_county.yml
name: Franklin County, Ohio
url: https://bids.franklincountyohio.gov/
type: static_html
selectors:
  table: table.sticky
  rows: tr
  title: td:nth-child(2) a
  due_date: td:nth-child(3)
  detail_link: td:nth-child(2) a[href]
```

### 2. ML-Based Selector Adaptation
Train model to automatically adapt to selector changes.

### 3. Scraper Marketplace
Allow community contributions for new municipalities.

### 4. Real-Time Scraping
Use webhooks/RSS feeds where available instead of polling.

---

## Questions?

Let me know which improvements you want to tackle first, and I can help implement them!
